Peer's tips:
1. figure out minikube good
2. metalLB good - look at website confiigure and install layer 2
3. use a docker image, so YAML file can pull the offical docker image of nginx files for nginx and congiure 
it with YAML files also in setup
https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/ something like this 
4. Change the docker pull image policy from always to if not available and make my own dockerfile for Nginx
5. repeat process 3 and 4 for other services

https://github.com/t0mm4rx/ft_services
https://github.com/Tishj/ft_services

Introduction (will save you some time !)
What are Docker and Kubernetes üê≥
Docker is a software that allow users to run lightweight virtual machines. You can build Docker "containers" 
with a Dockerfile. 

Container: Information and dtais of the contaner we wi be running. 
The image we will be running on the container, that is refered to by the tag you've given it
By defaut the image PullPolicy is set to always, since normaly you would pull images from a repository,
we will need to override ths and set it to never
There are many optional settings we can supplu as well, e.g:
- you can mount thing on the container sech as persistent columes or configmaps
- you can add envirment variables to the container
- you can run probes on the container to chec the heath or other state related things

A container is a single lightweight virtual machine running an os, with its own memory 
space and storage. It is created on an image, which is a template with preconfigured software. A container 
differs from a virtual machine because it uses the same kernel as the host computer, whereas a virtual 
machine has its own kernel. Containers are faster and lighter.

If you're running big apps that needs lot of containers/services, such as a database, web servers, monitoring tools, ftp, ssh..., you'll need a way to properly manage multiple Docker containers. It's not an easy task; you need to restart automatically crashed containers, to share data between them, to make sure some are fetchable from outside and some not... That's what Kubernetes does.

In Kubernetes, you have:

Deployment: You can view this as a group of one or more pods, based on its settings it will create one or more pods and manage their state (recreation in case of fail etc).
an object that runs and manages N instances of a given Docker image. For example, you can have a deployment that will launch and manage 10 Apache servers.

Service: an object that links a deployment externally or to other containers. For exemple, a deployment that will link the IP 192.168.0.1 to the 10 Apache servers and pick the one that has the least work load.

Pod: A pod is a running instance of a deployment, you can run a shell into it. It has its own IP and its own memory space.

Cluster: Collective term for the entirety of our running kubernetes setup (all machines/nodes).

Nodes: How kubernetes refers to physical/virtual machines that our cluster is running on.
All the above objects are described in YAML files.

secret: secrets et you tore sensitive information, such as user credential and passwords.
It can be referenced to create an environment variable from the secret.

Service: A reiable way to expose an application as a network service using an externall IP address or a clusterIP
(viirtual IP sed to communicate between pods).
It is possible to communicate between pods without a service, but you woud be required to have the clusterIP
of the ppod in question. By making a service and setting up the matchLabel selector swe can now connect to 
the clusterIP of the sercvice nby using the service name as that wil resolve to the clusterIP. (and that will in turn connect us through to the pod thats 
connected to the sservice)

LoadBalancer: connect t the interal loadbalancer (allb in our case) to request an external IP address for the service.
ClusterIP: default service, oonly used to communicate from pod to pod.
nodePort: bind the service to a port on the node'ss  externa IP (something we are nt alowed to use)

METALB layer 2 

Configmap: dedicated object used to store configuration settings, both for internal processes (metalLB), and also
configrations of our docker hosted application (nginx, phpmyadmin, wordpress etc).
Role: permissions, thats all they are. Permissions relation to the kubernetes API.
Rolebind: connecting the subject (an account, serviiceaccount etc..) to the permissions (the role).
Serviceaccoound:  User thats associated with pods, bby default there exiss one service account named 'default' 
that is automatically associated with every pod that gets created, if nothing else is specified.
PersistentVolume: sstorage that will persist between pod crashes, as its lifetime is independent of the pod.
PersistentColumeClaim: sed to bind a PersistedVolume with a pod.


Minikube is the software that we use to create a virtual machine that runs Kubernetes, and insures 
compatibility with VirtualBox. It features many tools, such as a dashboard to see how are you'r pods going.
MiniKube is a way tohost a kubernetes cluster, ths variant wil only have 1 node, as opposed to kubeadm
which is used to host a cluster consisting of multipple nodes.
When starting the minikube cluster we can suppy a bunch of arguments, some relating to resource usage
which you can osrt of tune to your liking.
What we are most interested in are the addons that minikube has to offer, all of these could be installed 
through different steps as well, but we are going to take advantage of the fact that we are running inikube
and enablethem from the start.

MetalLB: the LoadBalancer we will be using to create external IP addresses for us.
Dashboard: the kubernetes dashboard. (that we can open with command [minikube dashboard])
MetricsServer: This will make sure metrics are collected abut or custer and can be accessed through the kubernetes API.

[--extra-config=kubelet.authentication-token-webhook=true]
This allows us to authenticate our request to the kubernetes API with the servce account's bearer token that's 
placed into every container by default

COMMANDS:
Start minikube and create a cluster: 
[minikube start]

kubernetes deployment using existing image named echoserver
and exposed it on port 8080 using --port
[kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.10]

Check if the Pod is up and running
[kubectl get pod]

Get the URL of the exposed Service to view the Service details:
[minikube service hello-minikube --url]

Delete the hello-minikube Service:
[kubectl delete services hello-minikube]

Delete the hello-minikube Deployment:
[kubectl delete deployment hello-minikube]

Stop the local Minikube cluster:
[minikube stop]

Delete the local Minikube cluster:
[minikube delete]


Docker basics command ‚úÖ
# Build a docker image from a Dockerfile
docker build -t <your image name> <your Dockerfile dir>

# Start an instance of a docker image
docker run -it <your image name>
# Really important if you want to bind some ports on the container to your own computer, use -p option.
# Exemple for an Apache image using port 80 in the container as our port 80
docker run -it debian:apache -p80:80

# See all images
docker images

# See running containers
docker ps

# Stop a container
docker kill <container ID>

# Delete all unused Docker images and cache and free SO MUCH SPACE on your computer
docker system prune
How to manage pods with Kubernetes ‚úÖ
# Create a pod from a YAML file
kubectl create -f <yourfile.yaml>

# Delete a pod
kubectl delete deployment <your deployment>
kubectl delete service <your service>
# and so on if you have different objects

# Get a shell in a pod
# First get the pod full name with:
kubectl get pods
# Then, your pod name should look like "grafana-5bbf569f68-svdnz"
kubectl exec -it <pod name> -- /bin/sh

# Copy data to pod or to our computer
kubectl cp <pod name>:<file> <to>
# or vice versa
kubectl cp <from> <pod name>:<to>

# Restart a deployment
kubectl rollout restart deployment <name>

# Launch minikube dashboard
minikube dashboard

# Get cluster external IP
minikube ip

# Reset Minikube VM
minikube delete
How IPs are managed with Kubernetes ü§ñ
Kubernetes will create a network that connects all your containers. Each container will have its own private IP address. The network has an external IP. You can get it with "minikube ip". Sometimes, you want a container to connect another. For exemple, if you have a website in a container that needs a database from an other container, you need to create a service, which will create an easy-access to the database container.

From inside your Kubernetes network (from container to an other container), you can access a service by its name, and not its IP. For exemple, you have a service "mysql" linked to a MySQL container. To access this container from a Nginx container, you can try:

mysql <database> -u <user> -p -h mysql
# Normally, we access with IP like that:
mysql <database> -u <user> -p -h 127.0.0.10
An other example, you have a web page hosted on port 1000, with a service named "test". You minikube ip is 192.168.0.1.

# Access the webpage from containers
curl http://test:1000
# Access the webpage from outside (your computer, remotely)
curl http://192.168.0.1:1000
Link Minikube and Docker üîó
Minikube creates a specific VM in VirtualBox that will run your Docker images. You need to link your shell with the Minikube context. You can achieve that with the command:

eval $(minikube docker-env)
You can test in which context you are by running:

docker images
You can see all images linked to the current context that can help you identify were you are.

By default, Kubernetes deployment looks for online Docker images, but we want it to load our custom local images. You can do that by adding "imagePullPolicy: Never" prop in your container object.

Containers üßë‚Äçüíª
Nginx
Nginx is a web server that can provide web pages and execute PHP (a language for web backend). You need to create a simple Nginx server, it has to be fetchable through Ingres, which is a more advanced version of service. Port 443 is for SSL connection (https). You can create a SSL certificate with Openssl. This container needs to provide a SSH connection. SSH is used to access a computer remotly through a shell. A really simple way to create a SSH server is through the openssh package and then run the sshd daemon.

FTPs
A simple FTPs server. FTP is a protocol to send and download files from a distant computer. FTPs is a version that uses SSL to encrypt communications between the client and the server, which is safer. Pure-FTPD is a simple FTP server. You can test a FTP connection with:

ftp <user>@<ip>
Wordpress
Wordpress is the #1 open source website and blog content manager. It's written in PHP, and uses MySQL as database. MySQL is the most used SQL database, SQL is a language to query data. You'll need to use a web server, you can reuse Nginx. Your wordpress database (you'll need to import it in MySQL) contains the website IP information, which has to match the IP you access it from. You'll need to input the Minikube IP to the wordpress SQL database. Wordpress also has a wp-config.php file that you'll need to edit so it can access your MySQL service.

You can test a remote MySQL connection with:

# -p only if your user has a password
mysql <database name> -u <user> -p -h <ip>
PHPMyAdmin
PHPMyAdmin is a useful tool to view, query, and edit data from a MySQL database. It can be hosted by any web server, so I recommand you to use Nginx as well as you've used it before. You need to edit phpmyadmin.inc.php file to connect to your MySQL service.

Grafana
Grafana is a web dashboard used to visualize data, like a cluster health. It can automatically fetch data from various sources, but we'll use InfluxDB, which is a database engine.

You can test an InfluxDB connection by fecthing /ping endpoint:

curl http://influxdb:8086/ping
curl http://192.168.0.29/ping
We'll send all container data (CPU usage, memory, processes) easily by using Telegraf. It's a simple program that sends system data to an InfluxDB instance.

So our stack is: Telegraf --> InfluxDB --> Grafana Get data Store data Visualize Data

So there are two connections to configure, Telegraf to InfluxDB which is done in the /etc/telegraf/telegraf.conf file and the Grafana to InfluxDB which is done from the Grafana web interface.

To provide an already-configured version of Grafana, I advise you to setup a blank Grafana setup, launch you container, configure everything. Then save the grafana.db file on your computer (you can use "kubectl cp" to get data from a running pod). You can now copy this file in your Dockerfile.

Useful resources üï∏
To build Grafana + InfluxDB + Telegraf stack: https://medium.com/@nnilesh7756/copy-directories-and-files-to-and-from-kubernetes-container-pod-19612fa74660
Kubernetes cheat sheet: https://kubernetes.io/fr/docs/reference/kubectl/cheatsheet/





install matlb
install virtualbox
install minikube
try and get the nginx website from UVA environment working in minikube
remeber to do the rm/bin command thing for .minikube otherwise all gets full veryy quickly
setuup.sh contains the bin stuff for miniube and then start the minikube and then tell it to run all dockerfiles
see minikube addons can add those as well
all use alpine linux instead of debain buster in server


NEED To figure out the remove bin/ stuff because right now qinikube taking too much space to run*


YAML file for nginx with services and deployment
Nd using metalb


CONTAINERS:
- completely isolated environment with their own processes or services own networking interfaces own mounts like virtual machines but all share the same operating system kernel
Docker offers a high level tool for containerization
Operating systems all consists or an OS kernel and a set of softwares
Operating system kernel responsible for interacting with the underlying hardware 
The software above it is what makes these different
So all have same OS kernel and then different systems which differentiates systems from each other


KUBERNETES:
NODES: a node is a machine physical or virtual on which kubernetes is installed. It is a worker machine and is where containers will be launched on kubernetes
need more than one node so that if one goes down won't break everything
CLUSTER: cluster is a set of nodes. So even if one node fails application is still accessible from other nodes. Also helps in sharing loads.
MASTER: a node with kubernetes installed in it and is configured as the master. Master watches over the nodes in the cluster and is responsible of the actual orchestration of containers on the worker nodes

installing kubernetes is just installing: API server, etc, kubelet, container runtime, controller, scheduler.
- API server: acts as the front end for kubernetes , users management devices command line interfaces all talk to the API server to interact for the kubernetes cluster
- ETCD: key store, a distributor reliable key value store used to store all data to manage the cluster. Responsible for implementing locks within the cluster to ensure there are no conflicts within the masters
- scheduler: responsible for distributing work or containers across multiple nodes. 
Controllers
- The controllers are the brain behind orchestration. They are responsible for noticing
and responding when nodes, containers or endpoints goes down. The controllers
makes decisions to bring up new containers in such cases.
- The container runtime is the underlying software that is used to run containers. In our
case it happens to be Docker. 
- kubelet is the agent that runs on each node in the cluster. The agent is
responsible for making sure that the containers are running on the nodes as
expected.

Master vs worker node:
So far we saw two types of servers ‚Äì Master and Worker and a set of components
that make up Kubernetes. But how are these components distributed across different
types of servers. In other words, how does one server become a master and the
other slave?
The worker node (or minion) as it is also known, is were the containers are hosted.
For example Docker containers, and to run docker containers on a system, we need a
container runtime installed. And that‚Äôs were the container runtime falls. In this case it
happens to be Docker. This doesn‚Äôt HAVE to be docker.

The MASTER server has the kube-apiserver and that is what makes it a master. **
The WORKER nodes have the kubelet agent that is responsible for interacting
with the master to provide health information of the worker node and carry out
actions requested by the master on the worker nodes. 


All the information gathered are stored in a key-value store on the Master. The key
value store is based on the popular etcd framework as we just discussed.
The master also has the controller manager and the scheduler.
There are other components as well, but we will stop there for now. The reason we
went through this is to understand what components constitute the master and
worker nodes. This will help us install and configure the right components on
different systems when we setup our infrastructure. 

And finally, we also need to learn a little bit about ONE of the command line utilities
known as the kube command line tool or kubectl or kube control as it is also called.
The kube control tool is used to deploy and manage applications on a kubernetes
cluster, to get cluster information, get the status of nodes in the cluster and many
other things.
The kubectl run command is used to deploy an application on the cluster. The kubectl
cluster-info command is used to view information about the cluster and the kubectl
get pod command is used to list all the nodes part of the cluster. That‚Äôs all we need to
know for now and we will keep learning more commands throughout this course. We
will explore more commands with kubectl when we learn the associated concepts.
For now just remember the run, cluster-info and get nodes commands and that will
help us get through the first few labs

Minikube is a tool used to setup a single instance of Kubernetes in an All-in-one setup
and kubeadmin is a tool used to configure kubernetes in a multi-node setup. We will
look more into that in a bit. 


Minikube bundles all of these different components into a single image providing us a
pre-configured single node kubernetes cluster so we can get started in a matter of
minutes. 

You need virtual box installed

kubeadm tool which can be used to bootstrap a kubernetes cluster.
With the minikube utility you could only setup a single node kubernetes cluster. The
kubeadmin tool helps us setup a multi node cluster with master and workers on
separate machines. Installing all of these various components individually on different
nodes and modifying the configuration files to make it work is a tedious task.
Kubeadmin tool helps us in doing all of that very easily.
Let‚Äôs go through the steps ‚Äì First, you must have multiple systems or virtual
machines created for configuring a cluster. We will see how to setup up your laptop
to do just that if you are not familiar with it. Once the systems are created, designate
one as master and others as worker nodes.
The next step is to install a container runtime on the hosts. We will be using Docker,
so we must install Docker on all the nodes.
The next step is to install kubeadmin tool on all the nodes. The kubeadmin tool helps
us bootstrap the kubernetes solution by installing and configuring all the required
components in the right nodes.
The third step is to initialize the Master server. During this process all the required
components are installed and configured on the master server. That way we can start
the cluster level configurations from the master server.
Once the master is initialized and before joining the worker nodes to the master, we
must ensure that the network pre-requisites are met. A normal network connectivity
between the systems is not SUFFICIENT for this. Kubernetes requires a special
38
network between the master and worker nodes which is called as a POD network. We
will learn more about this network in the networking section later in this course. For
now we will simply follow the instructions available to get this installed and setup in
our environment.
The last step is to join the worker nodes to the master node. We are then all set to
launch our application in the kubernetes environment.

4. Just watched kubernetes setup - kubeadm




















